{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOw6oqBlmQqP7Idae7nnvHz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sTAzqCLJwnXA"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from itertools import count\n","from tqdm import tqdm\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["!pip install gymnasium\n","!pip install gymnasium[box2d]\n","!pip install jupyterlab\n","!pip install tqdm"],"metadata":{"id":"J5zvuUg3wqi_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","g_bins = 10\n","Q_track = 0\n","Q = 0"],"metadata":{"id":"74wLskSfw2l4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def create_bins(n_bins=g_bins, n_dim=4):\n","\n","    bins = [\n","        np.linspace(-4.8, 4.8, n_bins),\n","        np.linspace(-4, 4, n_bins),\n","        np.linspace(-0.418, 0.418, n_bins),\n","        np.linspace(-4, 4, n_bins)\n","    ]\n","\n","    return bins\n"],"metadata":{"id":"rz-X8m-pw5cr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discretize_state(observation, bins):\n","\n","    binned_state = []\n","\n","    for i in range(len(observation)):\n","        d = np.digitize(observation[i], bins[i])\n","        binned_state.append( d - 1)\n","\n","    return tuple(binned_state)"],"metadata":{"id":"YEKbU5NWw7rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decay_schedule(\n","    init_value, min_value, decay_ratio,\n","    max_steps, log_start = -2, log_base=10):\n","    decay_steps = int(max_steps*decay_ratio)\n","    rem_steps = max_steps - decay_steps\n","    values = np.logspace(\n","      log_start, 0, decay_steps,\n","      base = log_base, endpoint = True)[::-1]\n","    values = (values -values.min())/(values.max() - values.min())\n","    values = (init_value - min_value)*values +min_value\n","    values = np.pad(values, (0, rem_steps), 'edge')\n","\n","    return values"],"metadata":{"id":"tRxE2jexxB04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def generate_trajectory(\n","    select_action, Q, epsilon,\n","    env, max_steps=200):\n","    done, trajectory = False, []\n","    bins = create_bins(g_bins)\n","\n","    observation,_ = env.reset()\n","    state = discretize_state(observation, bins)\n","\n","    for t in count():\n","        action = select_action(state, Q, epsilon)\n","        observation, reward, done, _, _ = env.step(action)\n","        next_state = discretize_state(observation, bins)\n","        if not done:\n","            if t >= max_steps-1:\n","                break\n","            experience = (state, action,\n","                    reward, next_state, done)\n","            trajectory.append(experience)\n","        else:\n","            experience = (state, action,\n","                    -100, next_state, done)\n","            trajectory.append(experience)\n","            #time.sleep(2)\n","            break\n","        state = next_state\n","\n","    return np.array(trajectory, dtype=object)"],"metadata":{"id":"TfU3WiVwxD6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mc_control (env,n_bins=g_bins, gamma = 1.0,\n","                init_alpha = 0.5,min_alpha = 0.01, alpha_decay_ratio = 0.5,\n","                init_epsilon = 1.0, min_epsilon = 0.1, epsilon_decay_ratio = 0.9,\n","                n_episodes = 3000, max_steps = 200, first_visit = True, init_Q=None):\n","\n","    nA = env.action_space.n\n","    discounts = np.logspace(0, max_steps,\n","                            num = max_steps, base = gamma,\n","                            endpoint = False)\n","    alphas = decay_schedule(init_alpha, min_alpha,\n","                            0.9999, n_episodes)\n","    epsilons = decay_schedule(init_epsilon, min_epsilon,\n","                            0.99, n_episodes)\n","    pi_track = []\n","    global Q_track\n","    global Q\n","\n","\n","    if init_Q is None:\n","        Q = np.zeros([n_bins]*env.observation_space.shape[0] + [env.action_space.n],dtype =np.float64)\n","    else:\n","        Q = init_Q\n","\n","    n_elements = Q.size\n","    n_nonzero_elements = 0\n","\n","    Q_track = np.zeros([n_episodes] + [n_bins]*env.observation_space.shape[0] + [env.action_space.n],dtype =np.float64)\n","    select_action = lambda state, Q, epsilon: np.argmax(Q[tuple(state)]) if np.random.random() > epsilon else np.random.randint(len(Q[tuple(state)]))\n","\n","    progress_bar = tqdm(range(n_episodes), leave=False)\n","    steps_balanced_total = 1\n","    mean_steps_balanced = 0\n","    for e in progress_bar:\n","        trajectory = generate_trajectory(select_action, Q, epsilons[e],\n","                                    env, max_steps)\n","\n","        steps_balanced_total = steps_balanced_total + len(trajectory)\n","        mean_steps_balanced = 0\n","\n","        visited = np.zeros([n_bins]*env.observation_space.shape[0] + [env.action_space.n],dtype =np.float64)\n","        for t, (state, action, reward, _, _) in enumerate(trajectory):\n","            #if visited[tuple(state)][action] and first_visit:\n","            #    continue\n","            visited[tuple(state)][action] = True\n","            n_steps = len(trajectory[t:])\n","            G = np.sum(discounts[:n_steps]*trajectory[t:, 2])\n","            Q[tuple(state)][action] = Q[tuple(state)][action]+alphas[e]*(G - Q[tuple(state)][action])\n","        Q_track[e] = Q\n","        n_nonzero_elements = np.count_nonzero(Q)\n","        pi_track.append(np.argmax(Q, axis=env.observation_space.shape[0]))\n","        if e != 0:\n","            mean_steps_balanced = steps_balanced_total/e\n","        #progress_bar.set_postfix(episode=e, Epsilon=epsilons[e], Steps=f\"{len(trajectory)}\" ,MeanStepsBalanced=f\"{mean_steps_balanced:.2f}\", NonZeroValues=\"{0}/{1}\".format(n_nonzero_elements,n_elements))\n","        progress_bar.set_postfix(episode=e, Epsilon=epsilons[e], StepsBalanced=f\"{len(trajectory)}\" ,MeanStepsBalanced=f\"{mean_steps_balanced:.2f}\")\n","\n","    print(\"mean_steps_balanced={0},steps_balanced_total={1}\".format(mean_steps_balanced,steps_balanced_total))\n","    V = np.max(Q, axis=env.observation_space.shape[0])\n","    pi = lambda s:{s:a for s, a in enumerate(np.argmax(Q, axis=env.observation_space.shape[0]))}[s]\n","\n","    return Q, V, pi\n"],"metadata":{"id":"61mnQ1g0xG6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","observation, info = env.reset(seed=42)\n",""],"metadata":{"id":"fsFXja87xn1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%\n","env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","observation, info = env.reset(seed=42)\n","\n","# Train the MC control algorithm to get the Q array\n","Q_trained, V, pi = mc_control(env)\n","\n","# Save the trained Q array to a file\n","np.save(\"state_action_values.npy\", Q_trained)\n","\n","# %%\n","# Now you can load the Q array from the file\n","Q = np.load(\"state_action_values.npy\")"],"metadata":{"id":"EBKVPky0RPLd"},"execution_count":null,"outputs":[]}]}